{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def artcile_split_and_pred_para(sentences, predictions, add_one_pred=True):\n",
    "    'Retruns article splitted along with their sentences labels'\n",
    "    \n",
    "    if add_one_pred:\n",
    "        predictions.insert(0,'0\\n')\n",
    "    assert len(sentences) == len(predictions)\n",
    "    \n",
    "    article, article_splits, features = [], [], []\n",
    "    for i, (sent, pred) in enumerate(zip(sentences, predictions)):\n",
    "        if sent == 'ARTICLE_SPLIT_LINE\\t0\\n':\n",
    "            article_splits.append(article)\n",
    "            features.append(get_claim_premise_percentages(article))\n",
    "            article = []\n",
    "        else:\n",
    "            article.append(( sent.split('\\t')[0], int(pred.rstrip()) ))\n",
    "    \n",
    "    return article_splits, features\n",
    "def get_claim_premise_percentages(article):\n",
    "    'Return claim and premise sentences percentences of an article'\n",
    "    sent, claim, premise, none = 0, 0, 0, 0\n",
    "    \n",
    "    for _, label in article:\n",
    "        if label == 1:\n",
    "            claim += 1; sent +=1\n",
    "        elif label == 2:\n",
    "            premise += 1; sent +=1\n",
    "        else:\n",
    "            none += 1; sent += 1\n",
    "    \n",
    "    if sent == 0:\n",
    "        return 0, 0, 0\n",
    "    else:\n",
    "        return claim/sent, premise/sent, none/sent\n",
    "\n",
    "\n",
    "\n",
    "def artcile_split_and_pred_emb(sentences, predictions):\n",
    "    'Retruns article splitted along with their sentences labels'\n",
    "    predictions.insert(0,'0\\n')\n",
    "    article, article_splits = [], []\n",
    "    for i, (sent, pred) in enumerate(zip(sentences, predictions)):\n",
    "        if sent == 'ARTICLE_SPLIT_LINE\\t0\\n':\n",
    "            article_splits.append(article)\n",
    "            article = []\n",
    "        else:\n",
    "            article.append(( sent.split('\\t')[0], int(pred.rstrip()) ))\n",
    "    return article_splits\n",
    "def article_sent_labels_emb(article_sent_labels):\n",
    "    X = []\n",
    "    for _, label in article_sent_labels:\n",
    "        X.append(label)\n",
    "    return X\n",
    "def prepare_data(articles, sentences, predictions , mode='not_categorical'):\n",
    "    arg = artcile_split_and_pred_emb(sentences, predictions)\n",
    "    features, labels = [], []\n",
    "    for article_arg, article_lable in zip(arg, articles):\n",
    "        features.append(article_sent_labels_emb(article_arg))\n",
    "        labels.append(int(article_lable.split('\\t')[1].rstrip()))\n",
    "    if mode == 'categorical':\n",
    "        return np.array(features), np_utils.to_categorical(labels)\n",
    "    else: # mode == 'not_categorial'\n",
    "        return np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "def get_desired_layer_as_feature(embeddings, layers, get_sum=True):\n",
    "    '''Returns the selected layer as features from a given embeddings object of the top layers of [CLS] embeddings\n",
    "        layers : list of ids in [1,2,3,4], with a max length of 4\n",
    "        get_sum : True if sum, set to False if average of layers is desired'''\n",
    "    features = []\n",
    "    for item in embeddings:\n",
    "        CLS_sum = np.zeros(768)\n",
    "        for layer in layers:\n",
    "            CLS_sum += item['features'][0]['layers'][layer-1]['values']\n",
    "        if get_sum: # sum of layers\n",
    "            features.append(CLS_sum)\n",
    "        else: # average of layers\n",
    "            features.append(CLS_sum/len(layers))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,) <class 'numpy.ndarray'>\n",
      "(768,) <class 'numpy.ndarray'>\n",
      "(768,) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_articles = open('data/train.tsv').readlines()\n",
    "train_sent = open('data/train_sent_fixed/dev.tsv').readlines()\n",
    "train_pred = open('data/train_sent_fixed/predictions_editorial-claim-premise-bert.txt').readlines()\n",
    "\n",
    "dev_articles = open('data/dev.tsv').readlines()\n",
    "dev_sent = open('data/dev_sent_fixed/dev.tsv').readlines()\n",
    "dev_pred = open('../coling/data/dev_sent_fixed/predictions_editorial-claim-premise-bert.txt').readlines()\n",
    "\n",
    "test_articles = open('data/test/dev.tsv').readlines()\n",
    "test_sent = open('data/test_sent/dev.tsv').readlines()\n",
    "test_pred = open('data/test_sent/predictions_editorial-claim-premise-bert.txt').readlines()\n",
    "\n",
    "_, train_labels = prepare_data(train_articles, train_sent, train_pred)\n",
    "_, dev_labels = prepare_data(dev_articles, dev_sent, dev_pred)\n",
    "_, test_labels = prepare_data(test_articles, test_sent, test_pred)\n",
    "\n",
    "train_articles, train_features = artcile_split_and_pred_para(train_sent, train_pred, add_one_pred=False)\n",
    "dev_articles, dev_features = artcile_split_and_pred_para(dev_sent, dev_pred, add_one_pred=False)\n",
    "test_articles, test_features = artcile_split_and_pred_para(test_sent, test_pred, add_one_pred=False)\n",
    "\n",
    "\n",
    "train_embeddings = pickle.load(open('data/train.pkl','rb'))\n",
    "dev_embeddings = pickle.load(open('data/dev.pkl','rb'))\n",
    "test_embeddings = pickle.load(open('data/test/test.pkl','rb'))\n",
    "\n",
    "train_embeddings_features = np.array(get_desired_layer_as_feature(train_embeddings, [4]))\n",
    "dev_embeddings_features = np.array(get_desired_layer_as_feature(dev_embeddings, [4]))\n",
    "test_embeddings_features = np.array(get_desired_layer_as_feature(test_embeddings, [4]))\n",
    "\n",
    "print(train_embeddings_features[0].shape, type(train_embeddings_features))\n",
    "print(dev_embeddings_features[0].shape, type(dev_embeddings_features))\n",
    "print(test_embeddings_features[0].shape, type(test_embeddings_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3193\n",
      "           1       1.00      1.00      1.00      3193\n",
      "\n",
      "    accuracy                           1.00      6386\n",
      "   macro avg       1.00      1.00      1.00      6386\n",
      "weighted avg       1.00      1.00      1.00      6386\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96      3083\n",
      "           1       0.58      0.93      0.72       353\n",
      "\n",
      "    accuracy                           0.92      3436\n",
      "   macro avg       0.79      0.93      0.84      3436\n",
      "weighted avg       0.95      0.92      0.93      3436\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1387\n",
      "           1       0.86      0.82      0.84       418\n",
      "\n",
      "    accuracy                           0.93      1805\n",
      "   macro avg       0.90      0.89      0.89      1805\n",
      "weighted avg       0.92      0.93      0.93      1805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ta2509/miniconda3/envs/keras/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(train_embeddings_features, list(train_labels))\n",
    "pred = model.predict(train_embeddings_features)\n",
    "print(classification_report(list(train_labels), pred))\n",
    "\n",
    "\n",
    "pred = model.predict(dev_embeddings_features)\n",
    "print(classification_report(list(dev_labels), pred))\n",
    "\n",
    "pred = model.predict(test_embeddings_features)\n",
    "print(classification_report(list(test_labels), pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Dev and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuf_dev_emb, shuf_dev_features, shuf_dev_labels = shuffle(dev_embeddings_features, dev_features, list(dev_labels), random_state=293)\n",
    "dev_count, dev_limit = 0,353\n",
    "bal_dev_emb, bal_dev_features, bal_dev_labels = [], [], []\n",
    "\n",
    "\n",
    "for e,f,l in zip(shuf_dev_emb, shuf_dev_features, shuf_dev_labels):\n",
    "    if l ==0 and dev_count < dev_limit:\n",
    "        bal_dev_emb.append(e)\n",
    "        bal_dev_features.append(f)\n",
    "        bal_dev_labels.append(l)\n",
    "        dev_count += 1\n",
    "    elif l==1:\n",
    "        bal_dev_emb.append(e)\n",
    "        bal_dev_features.append(f)\n",
    "        bal_dev_labels.append(l)\n",
    "\n",
    "# bal_dev_emb, bal_dev_features = np.array(bal_dev_emb), np.array(bal_dev_features)\n",
    "\n",
    "\n",
    "shuf_test_emb, shuf_test_features, shuf_test_labels = shuffle(test_embeddings_features, test_features, list(test_labels), random_state=293)\n",
    "test_count, test_limit = 0, 418\n",
    "bal_test_emb, bal_test_features, bal_test_labels = [], [], []\n",
    "\n",
    "for e,f,l in zip(shuf_test_emb, shuf_test_features, shuf_test_labels):\n",
    "    if l ==0 and test_count < test_limit:\n",
    "        bal_test_emb.append(e)\n",
    "        bal_test_features.append(f)\n",
    "        bal_test_labels.append(l)\n",
    "        test_count += 1\n",
    "    elif l==1:\n",
    "        bal_test_emb.append(e)\n",
    "        bal_test_features.append(f)\n",
    "        bal_test_labels.append(l)\n",
    "\n",
    "# bal_test_emb, bal_test_features = np.array(bal_test_emb), np.array(bal_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       353\n",
      "           1       0.93      0.93      0.93       353\n",
      "\n",
      "    accuracy                           0.93       706\n",
      "   macro avg       0.93      0.93      0.93       706\n",
      "weighted avg       0.93      0.93      0.93       706\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90       418\n",
      "           1       0.96      0.82      0.88       418\n",
      "\n",
      "    accuracy                           0.89       836\n",
      "   macro avg       0.90      0.89      0.89       836\n",
      "weighted avg       0.90      0.89      0.89       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(bal_dev_emb)\n",
    "print(classification_report(list(bal_dev_labels), pred))\n",
    "\n",
    "pred = model.predict(bal_test_emb)\n",
    "print(classification_report(list(bal_test_labels), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ta2509/miniconda3/envs/keras/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3193\n",
      "           1       1.00      1.00      1.00      3193\n",
      "\n",
      "    accuracy                           1.00      6386\n",
      "   macro avg       1.00      1.00      1.00      6386\n",
      "weighted avg       1.00      1.00      1.00      6386\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96      3083\n",
      "           1       0.58      0.93      0.71       353\n",
      "\n",
      "    accuracy                           0.92      3436\n",
      "   macro avg       0.78      0.93      0.83      3436\n",
      "weighted avg       0.95      0.92      0.93      3436\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95      1387\n",
      "           1       0.85      0.82      0.83       418\n",
      "\n",
      "    accuracy                           0.92      1805\n",
      "   macro avg       0.90      0.89      0.89      1805\n",
      "weighted avg       0.92      0.92      0.92      1805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_combined_features = [list(emb)+list(arg) for emb, arg in zip(train_embeddings_features, train_features)]\n",
    "dev_combined_features = [list(emb)+list(arg) for emb, arg in zip(dev_embeddings_features, dev_features)]\n",
    "test_combined_features = [list(emb)+list(arg) for emb, arg in zip(test_embeddings_features, test_features)]\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "model.fit(train_combined_features, list(train_labels))\n",
    "pred = model.predict(train_combined_features)\n",
    "print(classification_report(list(train_labels), pred))\n",
    "\n",
    "\n",
    "pred = model.predict(dev_combined_features)\n",
    "print(classification_report(list(dev_labels), pred))\n",
    "\n",
    "pred = model.predict(test_combined_features)\n",
    "print(classification_report(list(test_labels), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       353\n",
      "           1       0.93      0.93      0.93       353\n",
      "\n",
      "    accuracy                           0.93       706\n",
      "   macro avg       0.93      0.93      0.93       706\n",
      "weighted avg       0.93      0.93      0.93       706\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90       418\n",
      "           1       0.95      0.82      0.88       418\n",
      "\n",
      "    accuracy                           0.89       836\n",
      "   macro avg       0.90      0.89      0.89       836\n",
      "weighted avg       0.90      0.89      0.89       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bal_dev_combined_features = [list(emb)+list(arg) for emb, arg in zip(bal_dev_emb, bal_dev_features)]\n",
    "bal_test_combined_features = [list(emb)+list(arg) for emb, arg in zip(bal_test_emb, bal_test_features)]\n",
    "\n",
    "pred = model.predict(bal_dev_combined_features)\n",
    "print(classification_report(list(bal_dev_labels), pred))\n",
    "\n",
    "pred = model.predict(bal_test_combined_features)\n",
    "print(classification_report(list(bal_test_labels), pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Dev/Test sets for Editorials and Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 140 140 Counter({1: 70, 0: 70})\n",
      "100 100 100 Counter({0: 50, 1: 50})\n",
      "548 548 548 Counter({0: 274, 1: 274})\n",
      "288 288 288 Counter({0: 144, 1: 144})\n"
     ]
    }
   ],
   "source": [
    "dev_set_ids = pickle.load(open('dev_set_ids_edi_let.p','rb'))\n",
    "dev_set_ids.keys()\n",
    "\n",
    "dev_features_edi = [f for i,f in enumerate(dev_features) if i in dev_set_ids['editorial_ids'] or i in dev_set_ids['editorial_news_ids']]\n",
    "dev_emb_features_edi = [f for i,f in enumerate(dev_embeddings_features) if i in dev_set_ids['editorial_ids'] or i in dev_set_ids['editorial_news_ids']]\n",
    "dev_labels_edi = [l for i,l in enumerate(dev_labels) if i in dev_set_ids['editorial_ids'] or i in dev_set_ids['editorial_news_ids']]\n",
    "\n",
    "dev_features_let = [f for i,f in enumerate(dev_features) if i in dev_set_ids['letter_ids'] or i in dev_set_ids['letter_news_ids']]\n",
    "dev_emb_features_let = [f for i,f in enumerate(dev_embeddings_features) if i in dev_set_ids['letter_ids'] or i in dev_set_ids['letter_news_ids']]\n",
    "dev_labels_let = [l for i,l in enumerate(dev_labels) if i in dev_set_ids['letter_ids'] or i in dev_set_ids['letter_news_ids']]\n",
    "\n",
    "print(len(dev_emb_features_edi), len(dev_features_edi), len(dev_labels_edi), Counter(dev_labels_edi))\n",
    "print(len(dev_emb_features_let), len(dev_features_let), len(dev_labels_let), Counter(dev_labels_let))\n",
    "\n",
    "\n",
    "test_set_ids = pickle.load(open('test_set_ids_edi_let.p','rb'))\n",
    "\n",
    "test_features_edi = [f for i,f in enumerate(test_features) if i in test_set_ids['editorial_ids'] or i in test_set_ids['editorial_news_ids']]\n",
    "test_emb_features_edi = [f for i,f in enumerate(test_embeddings_features) if i in test_set_ids['editorial_ids'] or i in test_set_ids['editorial_news_ids']]\n",
    "test_labels_edi = [l for i,l in enumerate(test_labels) if i in test_set_ids['editorial_ids'] or i in test_set_ids['editorial_news_ids']]\n",
    "\n",
    "test_features_let = [f for i,f in enumerate(test_features) if i in test_set_ids['other_ids'] or i in test_set_ids['other_news_ids']]\n",
    "test_emb_features_let = [f for i,f in enumerate(test_embeddings_features) if i in test_set_ids['other_ids'] or i in test_set_ids['other_news_ids']]\n",
    "test_labels_let = [l for i,l in enumerate(test_labels) if i in test_set_ids['other_ids'] or i in test_set_ids['other_news_ids']]\n",
    "\n",
    "print(len(test_emb_features_edi), len(test_features_edi), len(test_labels_edi), Counter(test_labels_edi))\n",
    "print(len(test_emb_features_let), len(test_features_let), len(test_labels_let), Counter(test_labels_let))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_features = [list(emb)+list(arg) for emb, arg in zip(train_embeddings_features, train_features)]\n",
    "# dev_combined_features = [list(emb)+list(arg) for emb, arg in zip(dev_embeddings_features, dev_features)]\n",
    "# test_combined_features = [list(emb)+list(arg) for emb, arg in zip(test_embeddings_features, test_features)]\n",
    "\n",
    "dev_combined_features_edi = [list(emb)+list(arg) for emb, arg in zip(dev_emb_features_edi, dev_features_edi)]\n",
    "dev_combined_features_let = [list(emb)+list(arg) for emb, arg in zip(dev_emb_features_let, dev_features_let)]\n",
    "\n",
    "test_combined_features_edi = [list(emb)+list(arg) for emb, arg in zip(test_emb_features_edi, test_features_edi)]\n",
    "test_combined_features_let = [list(emb)+list(arg) for emb, arg in zip(test_emb_features_let, test_features_let)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: SVM (Emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        70\n",
      "           1       0.93      0.93      0.93        70\n",
      "\n",
      "    accuracy                           0.93       140\n",
      "   macro avg       0.93      0.93      0.93       140\n",
      "weighted avg       0.93      0.93      0.93       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90       274\n",
      "           1       0.97      0.80      0.88       274\n",
      "\n",
      "    accuracy                           0.89       548\n",
      "   macro avg       0.90      0.89      0.89       548\n",
      "weighted avg       0.90      0.89      0.89       548\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92       144\n",
      "           1       0.97      0.85      0.90       144\n",
      "\n",
      "    accuracy                           0.91       288\n",
      "   macro avg       0.92      0.91      0.91       288\n",
      "weighted avg       0.92      0.91      0.91       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ta2509/miniconda3/envs/keras/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC()\n",
    "model.fit(train_embeddings_features, list(train_labels))\n",
    "pred = model.predict(train_embeddings_features)\n",
    "# print(classification_report(list(train_labels), pred))\n",
    "\n",
    "# Emb dev\n",
    "pred = model.predict(dev_emb_features_edi)\n",
    "print(classification_report(list(dev_labels_edi), pred))\n",
    "\n",
    "pred = model.predict(dev_emb_features_let)\n",
    "print(classification_report(list(dev_labels_let), pred))\n",
    "\n",
    "# Emb Test\n",
    "pred = model.predict(test_emb_features_edi)\n",
    "print(classification_report(list(test_labels_edi), pred))\n",
    "\n",
    "pred = model.predict(test_emb_features_let)\n",
    "print(classification_report(list(test_labels_let), pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: SVM (Emb+Arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        70\n",
      "           1       0.93      0.93      0.93        70\n",
      "\n",
      "    accuracy                           0.93       140\n",
      "   macro avg       0.93      0.93      0.93       140\n",
      "weighted avg       0.93      0.93      0.93       140\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89       274\n",
      "           1       0.97      0.79      0.87       274\n",
      "\n",
      "    accuracy                           0.89       548\n",
      "   macro avg       0.90      0.89      0.88       548\n",
      "weighted avg       0.90      0.89      0.88       548\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92       144\n",
      "           1       0.97      0.85      0.90       144\n",
      "\n",
      "    accuracy                           0.91       288\n",
      "   macro avg       0.92      0.91      0.91       288\n",
      "weighted avg       0.92      0.91      0.91       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ta2509/miniconda3/envs/keras/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "model.fit(train_combined_features, list(train_labels))\n",
    "pred = model.predict(train_combined_features)\n",
    "# print(classification_report(list(train_labels), pred))\n",
    "\n",
    "# Emb+Arg Dev\n",
    "pred = model.predict(dev_combined_features_edi)\n",
    "print(classification_report(list(dev_labels_edi), pred))\n",
    "\n",
    "pred = model.predict(dev_combined_features_let)\n",
    "print(classification_report(list(dev_labels_let), pred))\n",
    "\n",
    "# Emb+Arg Test\n",
    "pred = model.predict(test_combined_features_edi)\n",
    "print(classification_report(list(test_labels_edi), pred))\n",
    "\n",
    "pred = model.predict(test_combined_features_let)\n",
    "print(classification_report(list(test_labels_let), pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
